{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee08c9ce-13b8-44ae-9239-04213e4f8553",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama 3 70B IT\n",
    "This script fine-tunes and evaluates Llama 3 70B IT on a desired subset of fine-tuning datasets from the selection available in the repository (under \"results/fine-tuning_datasets\"), allowing for the recreation of results from the paper to be split up across multiple compute instances to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dfa73-e03e-40b5-b695-ae84a6253ac4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "You must select files you want to fine-tune and evaluate one-after-the-other on this compute instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9dd11-18ec-45f8-be7f-07448b6f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define functions that facilitate the selection process\n",
    "def list_files_recursive(directory):\n",
    "    file_list = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return sorted(file_list)\n",
    "\n",
    "def display_files(file_list):\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        print(f\"{i + 1}: {os.path.basename(file_path)}\")\n",
    "\n",
    "def parse_selection(selection_str, num_files):\n",
    "    selected_indices = set()\n",
    "    try:\n",
    "        parts = selection_str.split(',')\n",
    "        for part in parts:\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                start, end = int(start.strip()), int(end.strip())\n",
    "                if start > end:\n",
    "                    start, end = end, start\n",
    "                selected_indices.update(range(start-1, end))\n",
    "            else:\n",
    "                index = int(part.strip()) - 1\n",
    "                selected_indices.add(index)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter numbers and ranges correctly.\")\n",
    "    \n",
    "    # Filter out indices that are out of range\n",
    "    selected_indices = sorted(i for i in selected_indices if 0 <= i < num_files)\n",
    "    return selected_indices\n",
    "\n",
    "def get_user_selection(file_list):\n",
    "    selected_files = []\n",
    "    num_files = len(file_list)\n",
    "    selections = input(\"Enter the numbers or ranges of the files you want to select (e.g., 1-3,5,7-9): \")\n",
    "    selected_indices = parse_selection(selections, num_files)\n",
    "    \n",
    "    for index in selected_indices:\n",
    "        selected_files.append(file_list[index])\n",
    "    \n",
    "    return selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f313e8-40b9-4bff-9194-9f5db164c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_runs = 3  # Number of evaluation runs/trials to do after each model is fine-tuned\n",
    "directory = '../../results/fine-tuning_datasets'\n",
    "file_list = list_files_recursive(directory)\n",
    "display_files(file_list)\n",
    "\n",
    "if file_list:\n",
    "    selected_files = get_user_selection(file_list)\n",
    "    print(\"\\nYou selected the following files:\")\n",
    "    for file in selected_files:\n",
    "        print(os.path.basename(file))\n",
    "else:\n",
    "    selected_files = []\n",
    "    print(\"No files found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f002d5-6d21-4d03-8b29-077683d4cca8",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a8e7f-61d0-42ae-9504-5c01b330ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452fe18-8784-402d-828e-335301271483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "# Import helpers\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "# sys.path.append(os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__), '../../utils')))\n",
    "sys.path.append('../../utils')\n",
    "from useful_variables import languages, fine_tuning_eval_benchmarks\n",
    "from useful_functions import check_mc_answer, check_winogrande_answer\n",
    "evaluation_batch_path = '../../results/llm_evaluation/evaluation_batch_full.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e100dc-2658-4da5-b6c5-0e15161de42e",
   "metadata": {},
   "source": [
    "## Fine-Tuning Evaluation Loop\n",
    "This is the main loop that will iterate over the fine-tuning datasets and fine-tune Llama 3 70B IT on them, followed by evaluation in necessary benchmarks determined automatically from the fine-tuning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa29eb0-a5b4-4ea4-a6d0-6124fa15a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over files\n",
    "for fine_tuning_path in selected_files:\n",
    "    fine_tune_name = os.path.basename(fine_tuning_path)[:-6]  # create name that is distinct using FT dataset name\n",
    "    # Get language of tuning (and evaluation if quality/quantity is involved)\n",
    "    if 'language-is' in fine_tune_name:\n",
    "        this_language = fine_tune_name.split('_')[1]\n",
    "    else:\n",
    "        this_language = fine_tune_name.split('_')[-1]\n",
    "\n",
    "    # Get target eval benchmark if applicable\n",
    "    if 'test-is' in fine_tune_name:\n",
    "        this_eval_benchmark = fine_tune_name[fine_tune_name.find('test-is_')+len('test-is_'):fine_tune_name.find('_quality-is_')]\n",
    "    else:\n",
    "        this_eval_benchmark = None  # We will evaluate on all fine-tuning eval benchmarks in this case\n",
    "    print(f\"Now fine-tuning on \\\"{fine_tune_name}\\\", which is in language \\\"{this_language}\\\"...\")\n",
    "\n",
    "    # Load model fresh\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/llama-3-70b-Instruct-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Convert to PEFT\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 8,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 42,\n",
    "        use_rslora = False,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3\",\n",
    "    )\n",
    "    def formatting_prompts_func(examples):\n",
    "        convos = examples[\"messages\"]\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "        return { \"text\" : texts, }\n",
    "    train_df = pd.read_json(fine_tuning_path, lines=True)\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    train_ds = train_ds.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "    # Fine-Tune Llama 3 70B IT with SFT\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_ds,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 10,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 1,\n",
    "            # gradient_accumulation_steps = 4,\n",
    "            # warmup_steps = 5,\n",
    "            # max_steps = 60,\n",
    "            num_train_epochs = 3,\n",
    "            learning_rate = 2e-4,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 42,\n",
    "            output_dir = \"outputs\",\n",
    "        ),\n",
    "    )\n",
    "    # Print memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    # Actually fine-tune\n",
    "    trainer_stats = trainer.train()\n",
    "\n",
    "    print(\"Tuning finished!\")\n",
    "\n",
    "    # Print memory stats after tuning\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "    # Test inference\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3\",\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True, temperature = 0.7, top_p = 0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.batch_decode(outputs))  # Should say something like \"2+2=4\"\n",
    "\n",
    "    # Set output name prefix\n",
    "    output_name = f\"llama3-70b-instruct_{fine_tune_name}\"\n",
    "\n",
    "    # Define inference function that accepts a row in an OpenAI Batch API-formatted JSONL and produces a response\n",
    "    def infer(jsonl_row):\n",
    "        messages = jsonl_row['body']['messages']\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "    \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=jsonl_row['body']['max_tokens'],\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=jsonl_row['body']['temperature'],\n",
    "            top_p=jsonl_row['body']['top_p'],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "        response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    # Run small inference function test\n",
    "    test_row = {\"custom_id\": \"<|MODEL|>-on-en-mmlu-clinical_knowledge-0-answer-A\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"<|MODEL|>\", \"messages\": [{\"role\": \"user\", \"content\": \"The following are multiple choice questions (with answers) about clinical knowledge.\\n\\nQuestion 1: The energy for all forms of muscle contraction is provided by:\\nA. ATP.\\nB. ADP.\\nC. phosphocreatine.\\nD. oxidative phosphorylation.\\nAnswer: A\\n\\nQuestion 2: What is the difference between a male and a female catheter?\\nA. Male and female catheters are different colours.\\nB. Male catheters are longer than female catheters.\\nC. Male catheters are bigger than female catheters.\\nD. Female catheters are longer than male catheters.\\nAnswer: B\\n\\nQuestion 3: In the assessment of the hand function which of the following is true?\\nA. Abduction of the thumb is supplied by spinal root T2\\nB. Opposition of the thumb by opponens policis is supplied by spinal root T1\\nC. Finger adduction is supplied by the median nerve\\nD. Finger abduction is mediated by the palmar interossei\\nAnswer: B\\n\\nQuestion 4: How many attempts should you make to cannulate a patient before passing the job on to a senior colleague, according to the medical knowledge of 2020?\\nA. 4\\nB. 3\\nC. 2\\nD. 1\\nAnswer: C\\n\\nQuestion 5: Glycolysis is the name given to the pathway involving the conversion of:\\nA. glycogen to glucose-1-phosphate.\\nB. glycogen or glucose to fructose.\\nC. glycogen or glucose to pyruvate or lactate.\\nD. glycogen or glucose to pyruvate or acetyl CoA.\\nAnswer: C\\n\\nNow, given the following question and answer choices, output only the letter corresponding to the correct answer. Do not add any explanation.\\n\\nQuestion: What size of cannula would you use in a patient who needed a rapid blood transfusion (as of 2020 medical knowledge)?\\nA. 18 gauge.\\nB. 20 gauge.\\nC. 22 gauge.\\nD. 24 gauge.\\nAnswer:\\n\"}], \"max_tokens\": 3, \"temperature\": 0.7, \"top_p\": 0.9}}\n",
    "    print(infer(test_row))\n",
    "\n",
    "    \n",
    "    output_folder = '../../results/fine-tuning_experiments/'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run evaluation_runs many times\n",
    "    for i in range(evaluation_runs):\n",
    "        # Infer on necessary prompts\n",
    "        generations_map = {}\n",
    "        with open(evaluation_batch_path, 'r', encoding='utf-8') as fp:\n",
    "            all_prompts_jsonl = pd.read_json(fp, lines=True)\n",
    "            # Fine-tuned models are never evaluated on MMLU college medicine since it is used in training\n",
    "            all_prompts_jsonl = all_prompts_jsonl[~all_prompts_jsonl['custom_id'].str.contains('college_medicine')]\n",
    "            # Filter more if needed (target benchmark and language for quality x quantity versions)\n",
    "            if this_eval_benchmark is not None:\n",
    "                all_prompts_jsonl = all_prompts_jsonl[all_prompts_jsonl['custom_id'].str.contains(f'-on-{this_language}-{this_eval_benchmark}-')]\n",
    "                print(f\"Evaluating on {this_eval_benchmark} in {this_language} ({all_prompts_jsonl.shape[0]} questions)\")\n",
    "            else:\n",
    "                print(f\"Evaluating on everything except MMLU college medicine ({all_prompts_jsonl.shape[0]} questions)\")\n",
    "        \n",
    "        for index, row in tqdm(all_prompts_jsonl.iterrows(), total=all_prompts_jsonl.shape[0]):\n",
    "            try:\n",
    "                generations_map[row['custom_id'].replace('<|MODEL|>', output_name)] = infer({'custom_id': row['custom_id'], 'body': row['body']})\n",
    "            except Exception as e:\n",
    "                print(f\"Exception \\\"{e}\\\" occurred on \\\"{row['custom_id'].replace('<|MODEL|>', output_name)}\\\". Skipping...\")\n",
    "        \n",
    "        # Save generations so they never have to be run again\n",
    "        with open(os.path.join(output_folder, f'generations_{output_name}_{i}.json'), 'w') as fp:\n",
    "            json.dump(generations_map, fp, indent=2)\n",
    "\n",
    "    # Iterate over runs and place results in results folder\n",
    "    for i in range(evaluation_runs):\n",
    "        with open(os.path.join(output_folder, f'generations_{output_name}_{i}.json'), 'r') as fp:\n",
    "            generations_map = json.load(fp)\n",
    "\n",
    "        # Only get results if we actually tested on the benchmark for this fine-tuned model\n",
    "        if this_eval_benchmark is None or this_eval_benchmark == 'mmlu-clinical_knowledge':\n",
    "            # Get clinical knowledge performance (including more sections means taking the average perf. across the sections)\n",
    "            sections = [\n",
    "                'clinical_knowledge',\n",
    "            ]\n",
    "            \n",
    "            # initialize matrix of results\n",
    "            matrix = pd.DataFrame(\n",
    "                data=0.0,\n",
    "                index=[output_name],\n",
    "                columns=languages\n",
    "            )\n",
    "            matrix.index.name = \"Model\"\n",
    "        \n",
    "            for lang in languages:\n",
    "                # Accumulate scores and counts for average\n",
    "                total_score = 0\n",
    "                q_cnt = 0\n",
    "            \n",
    "                for section in sections:\n",
    "            \n",
    "                    # Construct the pattern\n",
    "                    pattern = re.compile(rf\".*-on-{lang}-mmlu-{section}.*\")\n",
    "            \n",
    "                    # Filter keys by pattern\n",
    "                    matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "            \n",
    "                    for (c_id, gen) in matching_generations:\n",
    "                        # Check correctness\n",
    "                        if check_mc_answer(c_id, gen):\n",
    "                            total_score += 1\n",
    "                        q_cnt += 1\n",
    "            \n",
    "                # Report error score if no matches found\n",
    "                if q_cnt == 0:\n",
    "                    final_score = -1\n",
    "                else:\n",
    "                    final_score = total_score / q_cnt\n",
    "                matrix.at[output_name, lang] = round(final_score*100, 1)\n",
    "            \n",
    "            # Save to CSV\n",
    "            matrix.to_csv(os.path.join(output_folder, f'{output_name}_on_mmlu-clinical_knowledge_{i}.csv'))\n",
    "\n",
    "        if this_eval_benchmark is None or this_eval_benchmark == 'mmlu-virology':\n",
    "            # Get virology performance (including more sections means taking the average perf. across the sections)\n",
    "            sections = [\n",
    "                'virology',\n",
    "            ]\n",
    "            \n",
    "            # initialize matrix of results\n",
    "            matrix = pd.DataFrame(\n",
    "                data=0.0,\n",
    "                index=[output_name],\n",
    "                columns=languages\n",
    "            )\n",
    "            matrix.index.name = \"Model\"\n",
    "        \n",
    "            for lang in languages:\n",
    "                # Accumulate scores and counts for average\n",
    "                total_score = 0\n",
    "                q_cnt = 0\n",
    "            \n",
    "                for section in sections:\n",
    "            \n",
    "                    # Construct the pattern\n",
    "                    pattern = re.compile(rf\".*-on-{lang}-mmlu-{section}.*\")\n",
    "            \n",
    "                    # Filter keys by pattern\n",
    "                    matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "            \n",
    "                    for (c_id, gen) in matching_generations:\n",
    "                        # Check correctness\n",
    "                        if check_mc_answer(c_id, gen):\n",
    "                            total_score += 1\n",
    "                        q_cnt += 1\n",
    "            \n",
    "                # Report error score if no matches found\n",
    "                if q_cnt == 0:\n",
    "                    final_score = -1\n",
    "                else:\n",
    "                    final_score = total_score / q_cnt\n",
    "                matrix.at[output_name, lang] = round(final_score*100, 1)\n",
    "            \n",
    "            # Save to CSV\n",
    "            matrix.to_csv(os.path.join(output_folder, f'{output_name}_on_mmlu-virology_{i}.csv'))\n",
    "\n",
    "        if this_eval_benchmark is None or this_eval_benchmark == 'winogrande':\n",
    "            # Get Winogrande performance\n",
    "            matrix = pd.DataFrame(\n",
    "                data=0.0,\n",
    "                index=[output_name],\n",
    "                columns=languages\n",
    "            )\n",
    "            matrix.index.name = \"Model\"\n",
    "            \n",
    "            for lang in languages:\n",
    "                # Accumulate scores and counts for average\n",
    "                total_score = 0\n",
    "                q_cnt = 0\n",
    "            \n",
    "                # Construct the pattern\n",
    "                pattern = re.compile(rf\".*-on-{lang}-winogrande.*\")\n",
    "            \n",
    "                # Filter keys by pattern\n",
    "                matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "            \n",
    "                for (c_id, gen) in matching_generations:\n",
    "                    # Check correctness\n",
    "                    if check_winogrande_answer(c_id, gen):\n",
    "                        total_score += 1\n",
    "                    q_cnt += 1\n",
    "            \n",
    "                # Report error score if no matches found\n",
    "                if q_cnt == 0:\n",
    "                    final_score = -1\n",
    "                else:\n",
    "                    final_score = total_score / q_cnt\n",
    "                matrix.at[output_name, lang] = round(final_score*100, 1)\n",
    "            \n",
    "            # Save to CSV\n",
    "            matrix.to_csv(os.path.join(output_folder, f'{output_name}_on_winogrande_{i}.csv'))\n",
    "\n",
    "        if this_eval_benchmark is None or this_eval_benchmark == 'belebele':\n",
    "            # Get Belebele performance\n",
    "            matrix = pd.DataFrame(\n",
    "                data=0.0,\n",
    "                index=[output_name],\n",
    "                columns=languages\n",
    "            )\n",
    "            matrix.index.name = \"Model\"\n",
    "        \n",
    "            for lang in languages:\n",
    "                # Accumulate scores and counts for average\n",
    "                total_score = 0\n",
    "                q_cnt = 0\n",
    "            \n",
    "                # Construct the pattern\n",
    "                pattern = re.compile(rf\".*-on-{lang}-belebele.*\")\n",
    "            \n",
    "                # Filter keys by pattern\n",
    "                matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "            \n",
    "                for (c_id, gen) in matching_generations:\n",
    "                    # Check correctness\n",
    "                    if check_mc_answer(c_id, gen):\n",
    "                        total_score += 1\n",
    "                    q_cnt += 1\n",
    "            \n",
    "                # Report error score if no matches found\n",
    "                if q_cnt == 0:\n",
    "                    final_score = -1\n",
    "                else:\n",
    "                    final_score = total_score / q_cnt\n",
    "                matrix.at[output_name, lang] = round(final_score*100, 1)\n",
    "            \n",
    "            matrix.to_csv(os.path.join(output_folder, f'{output_name}_on_belebele_{i}.csv'))\n",
    "        \n",
    "    # Clear model from memory to make room for next fine-tuning process\n",
    "    !rm -rf outputs\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45364a57-e2d2-4277-a1da-4350c31a2a35",
   "metadata": {
    "id": "45364a57-e2d2-4277-a1da-4350c31a2a35",
    "outputId": "6f2543a9-b0ae-4963-ec41-f6f739ce4d1e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248597c-108f-4069-8aa1-1b6d9b29487b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
