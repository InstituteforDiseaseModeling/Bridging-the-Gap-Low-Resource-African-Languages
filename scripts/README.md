# Scripts

This directory contains Python scripts and Jupyter Notebooks used to generate all results discussed in the paper. The scripts are organized into subdirectories corresponding to the experiments they are relevant to. The output of each script is saved in the [`results/`](../results) directory, in the subdirectory matching that of the script's location. The subdirectories are as follows:


[`scripts/figures/`](../scripts/figures): Contains Python scripts to generate the figures presented in the paper where possible from the raw data, where each script generates a single figure. The scripts are as follows:
- [`create_figure_1.py`](../scripts/figures/create_figure_1.py): Generates Figure 1. Outputs to [`results/figures/figure_1.svg`](../results/figures/figure_1.svg)
- [`create_figure_2.py`](../scripts/figures/create_figure_2.py): Generates Figure 2. Outputs to [`results/figures/figure_2.pdf`](../results/figures/figure_2.pdf)
- [`create_figure_3.py`](../scripts/figures/create_figure_3.py): Generates Figure 3. Outputs to [`results/figures/figure_3.pdf`](../results/figures/figure_3.pdf)
- [`create_figure_A10.py`](../scripts/figures/create_figure_A10.py): Generates Appendix Figure 10. Outputs to [`results/figures/figure_A10.pdf`](../results/figures/figure_A10.pdf)
- [`create_figure_A11.py`](../scripts/figures/create_figure_A11.py): Generates Appendix Figure 11. Outputs to [`results/figures/figure_A11.pdf`](../results/figures/figure_A11.pdf)
- [`create_figure_A13.py`](../scripts/figures/create_figure_A13.py): Generates Appendix Figure 13. Outputs to [`results/figures/figure_A13.pdf`](../results/figures/figure_A13.pdf)
- [`create_figure_A14.py`](../scripts/figures/create_figure_A14.py): Generates Appendix Figure 14. Outputs to [`results/figures/figure_A14.pdf`](../results/figures/figure_A14.pdf)
- [`create_figure_A16.py`](../scripts/figures/create_figure_A16.py): Generates Appendix Figure 16. Outputs to [`results/figures/figure_A16.pdf`](../results/figures/figure_A16.pdf)
- [`create_figure_A17.py`](../scripts/figures/create_figure_A17.py): Generates Appendix Figure 17. Outputs to [`results/figures/figure_A17.pdf`](../results/figures/figure_A17.pdf)
- [`create_figure_A18.py`](../scripts/figures/create_figure_A18.py): Generates Appendix Figure 18. Outputs to [`results/figures/figure_A18.pdf`](../results/figures/figure_A18.pdf)
- [`create_figure_A19.py`](../scripts/figures/create_figure_A19.py): Generates Appendix Figure 19. Outputs to [`results/figures/figure_A19.pdf`](../results/figures/figure_A19.pdf)
- [`create_figure_A20.py`](../scripts/figures/create_figure_A20.py): Generates Appendix Figure 20. Outputs to [`results/figures/figure_A20.pdf`](../results/figures/figure_A20.pdf)
- [`create_figure_A21.py`](../scripts/figures/create_figure_A21.py): Generates Appendix Figure 21. Outputs to [`results/figures/figure_A21.pdf`](../results/figures/figure_A21.pdf)

[`scripts/tables/`](../scripts/tables): Contains Python scripts to generate the tables presented in the paper where possible from the raw data. The scripts are as follows:
- [`create_table_1.py`](../scripts/tables/create_table_1.py): Generates Table 1. Outputs to [`results/tables/table_1.csv`](../results/tables/table_1.csv)
- [`create_table_2.py`](../scripts/tables/create_table_2.py): Generates Table 2. Outputs to [`results/tables/table_2.csv`](../results/tables/table_2.csv)
- [`create_table_A1.py`](../scripts/tables/create_table_A1.py): Generates Appendix Table 1. Outputs to [`results/tables/table_A1.csv`](../results/tables/table_A1.csv)
- [`create_table_A10.py`](../scripts/tables/create_table_A10.py): Generates Appendix Table 10. Outputs to [`results/tables/table_A10.csv`](../results/tables/table_A10.csv)
- [`create_tables_A11-A15.py`](../scripts/tables/create_tables_A11-A15.py): Generates Appendix Tables 11 through 15 (out-of-the-box performance on machine-translated benchmarks) as one CSV file. Outputs to [`results/tables/tables_A11-A15.csv`](../results/tables/tables_A11-A15.csv).
- [`create_table_A16.py`](../scripts/tables/create_table_A16.py): Generates Appendix Table 16. Outputs to [`results/tables/table_A16.csv`](../results/tables/table_A16.csv)
- [`create_table_A17.py`](../scripts/tables/create_table_A17.py): Generates Appendix Table 17. Outputs to [`results/tables/table_A17.csv`](../results/tables/table_A17.csv)
- [`create_table_A18.py`](../scripts/tables/create_table_A18.py): Generates Appendix Table 18. Outputs to [`results/tables/table_A18.csv`](../results/tables/table_A18.csv)
- [`create_table_A19.py`](../scripts/tables/create_table_A19.py): Generates Appendix Table 19. Outputs to [`results/tables/table_A19.csv`](../results/tables/table_A19.csv)
- [`create_tables_A20-A27.py`](../scripts/tables/create_tables_A20-A27.py): Generates Appendix Tables 20 through 27 (cross-lingual tuning results) as two CSV files, one for the means and one for the standard deviations (three evaluation trials were performed). Outputs to [`results/tables/tables_A20-A27_means.csv`](../results/tables/tables_A20-A27_means.csv) and [`results/tables/tables_A20-A27_stds.csv`](../results/tables/tables_A20-A27_stds.csv).
- [`create_tables_A28-A31.py`](../scripts/tables/create_tables_A28-A31.py): Generates Appendix Tables 28 through 31 (quality x quantity tuning results), split up into multiple files, where each file represents a 2x5 colored section of numbers in one of these appendix tables. Outputs each CSV file to [`results/tables/tables_A28-A31/`](../results/tables/tables_A28-A31), which contains files of the form `train-on_<ft_dataset>_eval-on_<eval_dataset>_in_<language>_means.csv` and `train-on_<ft_dataset>_eval-on_<eval_dataset>_in_<language>_stds.csv`. Here,
  - `<ft_dataset>` is one of `mmlu-college_medicine` or `winogrande` indicating the fine-tuning dataset used.
  - `<eval_dataset>` is one of `belebele`, `mmlu-clinical_knowledge`, `mmlu-virology`, or `winogrande` indicating the evaluation dataset tested as well as used as the target benchmark for quality annotations by GPT-4o.
  - `<language>` is the [ISO language code](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) indicating the language of fine-tuning and evaluation for the table.

*Note that Appendix Table 25 is sourced directly from [`data/translations_and_llm_responses/4. Winogrande Upworker Profiles.csv`](<../data/translations_and_llm_responses/4. Winogrande Upworker Profiles.csv>).*


[`scripts/fine-tuning_datasets/`](../scripts/fine-tuning_datasets): Contains Python scripts that create fine-tuning datasets in [OpenAI Fine-Tuning API](https://platform.openai.com/docs/guides/fine-tuning) format (but can be used by Hugging Face models as well with our scripts). Here is a breakdown of the scripts:
- [`create_fine-tuning_datasets_full.py`](fine-tuning_datasets/create_fine-tuning_datasets_full.py): Creates the fine-tuning dataset `.jsonl` files used to produce the cross-lingual results for the paper. Outputs are in [`results/fine-tuning_datasets/full/`](../results/fine-tuning_datasets/full) and labeled `<fine-tuning_dataset>_<language>.jsonl`
- [`create_fine-tuning_datasets_quality_x_quantity.py`](fine-tuning_datasets/create_fine-tuning_datasets_quality_x_quantity.py): Creates the fine-tuning dataset `.jsonl` files used to produce the quality vs quantity results for the paper. Outputs are in [`results/fine-tuning_datasets/quality_x_quantity/`](../results/fine-tuning_datasets/quality_x_quantity) and labeled `language-is_<language>_train-is_<fine-tuning_dataset>_test-is_<evaluation_benchmark>_quality-is_<low or high>_quantity-percent-is_<25, 50, 75, or 100>.jsonl`. Note that these are not the exact quality x quantity fine-tuning datasets used in our experiments (due to GPT-4o generation randomness), but they were generated using the same method. For the exact fine-tuning datasets we used, see the `data/` folder [README](../data/README.md) under `data/translations_and_llm_responses/7. Fine-Tuning Datasets.csv`.

[`scripts/fine-tuning_experiments/`](../scripts/fine-tuning_experiments): Contains a Jupyter Notebook and identical Python script **(the latter of which must be run with `ipython` and not `python3` or `python`)** that fine-tunes and evaluates Llama 3 70B IT on desired fine-tuning datasets in [`results/fine-tuning_datasets/`](../results/fine-tuning_datasets) to rerun the cross-lingual and quality vs quantity experiments performed for our paper. Here is a breakdown of the scripts:
- `fine_tune_llama3_70b_instruct.<ipynb or py>`: An interactive script that prompts the user to select fine-tuning datasets and subsequently fine-tunes Llama 3 70B IT on them one-at-a-time and evaluates the resulting fine-tuned models on the necessary benchmarks to reproduce the results from our paper. Note that these scripts must be run in the [`scripts/fine-tuning_experiments/`](fine-tuning_experiments) folder, unlike most other scripts which may be run from anywhere. **This script will likely cost money to run as it requires a powerful GPU like an A100 (which we used).** Example outputs (where the "LLM" always said "A") for cross-lingual and quality x quantity experiments are in [`results/fine-tuning_experiments/`](../results/fine-tuning_experiments), which is where the real outputs would be sent. The outputs are named as following:
  - `generations_llama3-70b-instruct_<fine-tuning dataset name without .jsonl>_<trial number starting at 0>.json`: The generations file of a given evaluation run giving the responses of the fine-tuned model on each tested evaluation benchmark question.
  - `llama3-70b-instruct_<fine-tuning dataset name without .jsonl>_on_<evaluation benchmark>_<trial number starting at 0>.csv`: Contains the accuracy scores of the fine-tuned model on the `<evaluation_benchmark>` for each language (using a `-100.0` placeholder if a language was not tested).

[`scripts/gpt_performance/`](../scripts/gpt_performance): The script within ([`parse_gpt_performance_results.py`](../scripts/gpt_performance/parse_gpt_performance_results.py)) parses the GPT-3.5, GPT-4, and GPT-4o out-of-the-box `.jsonl` Winogrande output from [`data/gpt_performance/`](../data/gpt_performance) and generates the data tables in [`results/gpt_performance/`](../results/gpt_performance) which can then be used to calculate performance metrics for the GPT models.

[`scripts/llm-as-an-annotator/`](../scripts/llm-as-an-annotator): Contains Python scripts that generate GPT-4o LLM-as-an-Annotator quality scores for our fine-tuning datasets formed from the evaluation benchmarks. Here is a breakdown of the scripts:
- [`create_quality_batches.py`](llm-as-an-annotator/create_quality_batches.py): Creates [OpenAI Batch API](https://platform.openai.com/docs/guides/batch) `.jsonl` files (split into two equal pieces to abide by the 50,000 request per batch limit) that prompt GPT-4o to judge the quality of our fine-tuning dataset rows on the same language/fine-tuning dataset/evaluation benchmark combinations as in the paper (5% or greater mono-lingual lift observed when using full fine-tuning dataset). Outputs to [`results/llm-as-an-annotator/gpt-4o_quality_batch_0.jsonl`](../results/llm-as-an-annotator/gpt-4o_quality_batch_0.jsonl) and [`results/llm-as-an-annotator/gpt-4o_quality_batch_1.jsonl`](../results/llm-as-an-annotator/gpt-4o_quality_batch_1.jsonl)
- [`send_quality_batches.py`](llm-as-an-annotator/send_quality_batches.py): Sends the `.jsonl` files created by [`create_quality_batches.py`](llm-as-an-annotator/create_quality_batches.py) to the [OpenAI Batch API](https://platform.openai.com/docs/guides/batch). **This script requires a valid [OpenAI API key](../README.md#2-set-up-configpy-file-gitignored-by-default-containing-secrets) and costs money to run.** Downloads the results to [`results/llm-as-an-annotator/quality_generations_gpt-4o_0.jsonl`](../results/llm-as-an-annotator/quality_generations_gpt-4o_0.jsonl) and [`results/llm-as-an-annotator/quality_generations_gpt-4o_1.jsonl`](../results/llm-as-an-annotator/quality_generations_gpt-4o_1.jsonl)
- [`map_benchmarks_to_quality_scores.py`](llm-as-an-annotator/map_benchmarks_to_quality_scores.py): Parses the quality responses by GPT-4o produced by [`send_quality_batches.py`](llm-as-an-annotator/send_quality_batches.py) and creates a mapping of benchmark row/language combinations to GPT-4o quality scores in [`results/llm-as-an-annotator/benchmarks_to_quality_scores.json`](../results/llm-as-an-annotator/benchmarks_to_quality_scores.json).
- [`get_quality_buckets.py`](llm-as-an-annotator/get_quality_buckets.py): Takes the mapping in [`results/llm-as-an-annotator/benchmarks_to_quality_scores.json`](../results/llm-as-an-annotator/benchmarks_to_quality_scores.json) and creates a mapping of language/fine-tuning dataset/evaluation dataset combinations to lists of fine-tuning dataset rows (using IDs) by quality, effectively forming the quality buckets in [`results/llm-as-an-annotator/quality_buckets.json`](../results/llm-as-an-annotator/quality_buckets.json). This script also produces visualizations of the quality score distributions in [`results/llm-as-an-annotator/plots/`](../results/llm-as-an-annotator/plots)

[`scripts/llm_evaluation/`](../scripts/llm_evaluation): Contains Python scripts that recreate evaluation batch `.jsonl` files that are used to evaluate LLMs (fine-tuned or otherwise) on our evaluation benchmarks. Here is a breakdown of the scripts:
- `create_evaluation_batch_full<suffix>.py`: Creates a GPT-style Batch API-like `.jsonl` file containing all questions from our evaluation benchmarks with prompts added so they can be given to LLMs for evaluation. `<suffix>` gives the translation approach used for the dataset, with `_mt` meaning machine-translated into the target language, `_bt` meaning backtranslated from the human translations into English, and no suffix meaning human-translated. Results are output to `results/llm_evaluation/evaluation_batch_full<suffix>.jsonl`.

[`scripts/out-of-the-box/`](../scripts/out-of-the-box): Contains Python scripts and Jupyter Notebooks to reproduce the results of evaluating various models out-of-the-box (no fine-tuning) on our translated benchmarks. **Note that these scripts typically cost money to run,** as it is likely an A100 GPU (what we used) or so will have to be rented or the [OpenAI Batch API](https://platform.openai.com/docs/guides/batch) will have to be used.

In addition, these scripts must be **run from the [`scripts/out-of-the-box/`](../scripts/out-of-the-box) folder**, unlike most other scripts, which may be run from any directory.

The Jupyter Notebook scripts have corresponding Python scripts which are functionally identical and may be run with
```shell
ipython <script name>.py  # Notice that "ipython" is used, not "python3" or "python"
```

*Note that to evaluate on machine-translated versions of the benchmarks, replace "evaluation_batch_full" with "evaluation_batch_full_mt", or "evaluation_batch_full_bt" to evaluate on machine backtranslations in English.*

Here is a breakdown of the scripts in this folder:
- `evaluate_<model_name>_ootb.<ipynb or py>` *(non-GPT models)*: Evaluates `<model_name>` on our translated evaluation benchmarks and outputs to [`results/out-of-the-box/`](../results/out-of-the-box) with a `.json` file called `generations_<model_name>.json` listing the results on each evaluation benchmark question and CSV files called `<model_name>_on_<evaluation_benchmark>.csv` listing the performance in each language on `<evaluation_benchmark>`. Example output files are provided in [`results/out-of-the-box/`](../results/out-of-the-box). Note that to use Aya 23 35B, you will need to have a Hugging Face read token to access it (instructions are in the actual Aya 23 35B out-of-the-box scripts).
- `evaluate_<model_name>_ootb.py` *(GPT models)*: Evaluates `<model_name>` on our translated evaluation benchmarks and outputs to [`results/out-of-the-box/`](../results/out-of-the-box) with a `.jsonl` [OpenAI Batch API](https://platform.openai.com/docs/guides/batch) output file called `generations_<model_name>.jsonl` listing the results on each evaluation benchmark question. Example output files are provided in [`results/out-of-the-box/`](../results/out-of-the-box). Note that using these scripts requires a valid OpenAI API key (see the main [README](../README.md#2-set-up-configpy-file-gitignored-by-default-containing-secrets)). 
- [`process_gpt_generations.py`](out-of-the-box/process_gpt_generations.py): Takes a desired generations `.jsonl` file for a GPT model and produces CSV files called `<desired_model_name>_on_<evaluation_benchmark>.csv` listing the performance in each language on `<evaluation_benchmark>`. Example output files are provided in [`results/out-of-the-box/`](../results/out-of-the-box).

[`scripts/statistics/`](../scripts/statistics): Contains Python scripts to generate key statistics that are mentioned in the paper. The scripts are as follows:
- [`get_gain_statistics.py`](../scripts/statistics/get_gain_statistics.py): Computes statistics related to gains above baselines for mono-lingual, cross-lingual, and quantity x quality fine-tuning experiments. Outputs to [`results/statistics/gain_statistics.txt`](../results/statistics/gain_statistics.txt) as free-form text.
- [`get_word_count_of_translations.py`](../scripts/statistics/get_word_count_of_translations.py): Computes statistics related to word counts of the datasets we translated for this project. Outputs to [`results/statistics/word_count_of_translations.txt`](../results/statistics/word_count_of_translations.txt) as free-form text. Note that counts for translations that were pre-existing will be 0.

[`scripts/o1-related/`](../scripts/o1-related): Contains Python scripts added after the paper was submitted that test OpenAI o1 models (e.g. o1-mini, o1-preview). Note that these models were released after the AAAI paper deadline.
- [`evaluate_o1-mini_ootb.py`](../scripts/o1-related/evaluate_o1-mini_ootb.py): Evaluates o1-mini out-of-the-box on all benchmarks that the other models were evaluated on. Since o1-mini does not have Batch API support as of writing, an equivalent approach was used with the Completions API. 
This script is the most expensive one in the repository, costing around $325. o1-preview evaluations were omitted since they would likely cost around 5x that, but this script can easily be modified to evaluate o1-preview by changing the `MODEL` variable at the top. Outputs raw responses to [`results/o1-related/generations_o1-mini-2024-09-12.jsonl`](../results/o1-related/generations_o1-mini-2024-09-12.jsonl). Since this script is expensive, it is designed to save results as they come in and remember results that it has already computed. So, if any errors occur, simply rerun the script and it will start where it left off and/or try again on ones it failed on previously.
- [`process_o1_generations.py`](../scripts/o1-related/process_o1_generations.py): Takes the generations produced from [`evaluate_o1-mini_ootb.py`](../scripts/o1-related/evaluate_o1-mini_ootb.py) and gets the scores on each benchmark for each evaluation benchmark in each language as CSV files. Outputs to `results/o1-related/o1-mini-2024-09-12_on_<evaluation_benchmark>.csv` for each of the five evaluation benchmarks.
- [`add_o1-mini_to_table_2.py`](../scripts/o1-related/add_o1-mini_to_table_2.py): Takes Table 2's (out-of-the-box performance on human-translated benchmarks) CSV file and adds the o1-mini results above the GPT-4o ones. We found that GPT-4o still remained superior across all benchmarks. Outputs to [`results/o1-related/table_2_with_o1-mini.csv`](../results/o1-related/table_2_with_o1-mini.csv)
- [`estimate_o1_costs.py`](../scripts/o1-related/estimate_o1_costs.py): Runs a very small test that was used to estimate the cost of evaluating o1-mini (and o1-preview) out-of-the-box. Outputs a report to [`results/o1-related/cost_estimations.txt`](../results/o1-related/cost_estimations.txt). In the end, the mean estimated o1-mini cost ($324.11) was less than $2 away from the actual o1-mini evaluation cost ($325.93, observed using our billing account).
